## Rules to format the taxonomy into a format for DADA2 implementation of RDP.
scripts = os.path.join(workflow.basedir, "rules", "DB_processing", "scripts")


rule qiime2_import_fasta:
    conda:
        os.path.join(dir.envs, "qiime2.yml")
    container:
        singularity_envs["qiime2"]
    input:
        os.path.join("{prefix}", "database", "amplicons.fasta"),
    output:
        os.path.join("{prefix}", "qiime2", "amplicons.qza"),
    log:
        os.path.join("{prefix}", "logs", "qiime2", "import_fasta.txt"),
    shell:
        """
        qiime tools import \\
        --input-path {input} \\
        --output-path {output} \\
        --type 'FeatureData[Sequence]' &> {log}
        """


rule qiime2_import_taxonomy:
    conda:
        os.path.join(dir.envs, "qiime2.yml")
    container:
        singularity_envs["qiime2"]
    input:
        os.path.join("{prefix}", "database", "amplicons_tax.tsv"),
    output:
        os.path.join("{prefix}", "qiime2", "amplicons_tax.qza"),
    log:
        os.path.join("{prefix}", "logs", "QIIME", "import_tax.txt"),
    shell:
        """
        qiime tools import \\
        --input-path {input} \\
        --output-path {output} \\
        --type 'FeatureData[Taxonomy]' \\
        --input-format HeaderlessTSVTaxonomyFormat &> {log}
        """


rule qiime2_fit_classifier_naive_bayes:
    conda:
        os.path.join(dir.envs, "qiime2.yml")
    container:
        singularity_envs["qiime2"]
    input:
        fa=os.path.join("{prefix}", "qiime2", "amplicons.qza"),
        tax=os.path.join("{prefix}", "qiime2", "amplicons_tax.qza"),
    output:
        os.path.join("{prefix}", "qiime2", "rdp_classifier.qza"),
    log:
        os.path.join("{prefix}", "logs", "qiime2", "rdp_train.txt"),
    shell:
        """
        qiime feature-classifier fit-classifier-naive-bayes \\
        --i-reference-reads {input.fa} \\
        --i-reference-taxonomy {input.tax} \\
        --o-classifier {output}
        """


rule Hash_qiime2:
    input:
        os.path.join("{prefix}", "qiime2", "amplicons.qza"),
        os.path.join("{prefix}", "qiime2", "amplicons_tax.qza"),
        os.path.join("{prefix}", "qiime2", "rdp_classifier.qza"),
    output:
        os.path.join("{prefix}", "qiime2", "qiime2.md5"),
    shell:
        """
        md5sum {input} | sort -k 2 | md5sum | cut -f 1 -d " " > {output}
        """


rule get_genus_tax:
    input:
        os.path.join("{prefix}", "database", "amplicons_tax.tsv"),
    output:
        temp(os.path.join("{prefix}", "database", "amplicons_to_genus_tax.tsv")),
    shell:
        """
        sed -E 's/(.*);[^;]*$/\\1/' {input} > {output}
        """


rule get_species_assign_tax:
    input:
        os.path.join("{prefix}", "database", "amplicons_tax.tsv"),
    output:
        temp(os.path.join("{prefix}", "database", "amplicons_sp_assign_tax.tsv")),
    shell:
        """
        sed 's/.*;\\([^;]* [^;]*\\)$/\\1/' {input} > {output}
        """


rule get_dada2_trainset_fastas:
    conda:
        os.path.join(dir.envs, "seqkit.yml")
    container:
        singularity_envs["seqkit"]
    input:
        fa=os.path.join("{prefix}", "database", "amplicons.fasta"),
        to_gen=os.path.join("{prefix}", "database", "amplicons_to_genus_tax.tsv"),
        to_sp=os.path.join("{prefix}", "database", "amplicons_tax.tsv"),
        sp_assign=os.path.join("{prefix}", "database", "amplicons_sp_assign_tax.tsv"),
    output:
        to_gen=os.path.join("{prefix}", "dada2", "toGenus_trainset.fa.gz"),
        to_sp=os.path.join("{prefix}", "dada2", "toSpecies_trainset.fa.gz"),
        sp_assign=os.path.join("{prefix}", "dada2", "assignSpecies.fa.gz"),
    log:
        os.path.join("{prefix}", "logs", "dada2", "trainset_fasta.log"),
    shell:
        """
        seqkit replace \\
        -p "^(.+)$" -r "{{kv}}" \\
        -k {input.to_gen} \\
        {input.fa} \\
        -o {output.to_gen} \\
        2>> {log}

        seqkit replace \\
        -p "^(.+)$" -r "{{kv}}" \\
        -k {input.to_sp} \\
        {input.fa} \\
        -o {output.to_sp} \\
        2>> {log}

        seqkit replace \\
        -p '^(.+)$' -r '\\1 {{kv}}' \\
        -k {input.sp_assign} \\
        {input.fa} \\
        -o {output.sp_assign} \\
        2>> {log}
        """


rule Hash_DADA2:
    input:
        os.path.join("{prefix}", "dada2", "toGenus_trainset.fa.gz"),
        os.path.join("{prefix}", "dada2", "toSpecies_trainset.fa.gz"),
        os.path.join("{prefix}", "dada2", "assignSpecies.fa.gz"),
    output:
        os.path.join("{prefix}", "dada2", "dada2.md5"),
    shell:
        """
        md5sum {input} | sort -k 2 | md5sum | cut -f 1 -d " " > {output}
        """


## Rules fo sintax classifier


rule get_sintax_formatted_tax:
    input:
        os.path.join("{prefix}", "database", "amplicons_tax.tsv"),
    output:
        temp(os.path.join("{prefix}", "sintax", "sintax_headers.tsv")),
    params:
        RANKS,
    run:
        ranks = params[0].split(",")
        df = pd.read_csv(input[0], sep="\t", header=None)
        df.columns = ["seq_id", "tax"]
        df["sintax"] = df.apply(lambda row: get_sintax_tax(row, ranks), axis=1)
        df[["seq_id", "sintax"]].to_csv(output[0], sep="\t", header=False, index=False)


rule get_sintax_trainset:
    conda:
        os.path.join(dir.envs, "seqkit.yml")
    container:
        singularity_envs["seqkit"]
    input:
        fa=os.path.join("{prefix}", "database", "amplicons.fasta"),
        tsv=os.path.join("{prefix}", "sintax", "sintax_headers.tsv"),
    output:
        os.path.join("{prefix}", "sintax", "trainset_sp.fa.gz"),
    log:
        os.path.join("{prefix}", "logs", "sintax", "trainset_fasta.log"),
    shell:
        """
        seqkit replace \\
        -p "^(.+)$" -r "{{kv}}" \\
        -k {input.tsv} \\
        {input.fa} \\
        -o {output} \\
        2> {log}
        """


rule Hash_sintax:
    input:
        os.path.join("{prefix}", "sintax", "trainset_sp.fa.gz"),
    output:
        os.path.join("{prefix}", "sintax", "sintax.md5"),
    shell:
        """
        md5sum {input} | sort -k 2 | md5sum | cut -f 1 -d " " > {output}
        """


## Rules fo kraken2 classifier


rule get_kraken2_taxonomy:
    input:
        os.path.join("{prefix}", "database", "amplicons_tax.tsv"),
    output:
        temp(os.path.join("{prefix}", "kraken2", "seqid2tax.tsv")),
    params:
        RANKS,
    run:
        ranks = params[0].split(",")
        df = pd.read_csv(input[0], sep="\t", header=None)
        df.columns = ["seq_id", "tax"]
        df[ranks] = df["tax"].str.split(";", expand=True)
        df[["seq_id"] + ranks].to_csv(output[0], sep="\t", index=False)


rule get_kraken2_taxdump:
    conda:
        os.path.join(dir.envs, "taxonkit.yml")
    container:
        singularity_envs["taxonkit"]
    input:
        os.path.join("{prefix}", "kraken2", "seqid2tax.tsv"),
    output:
        os.path.join("{prefix}", "kraken2", "taxonomy", "names.dmp"),
        os.path.join("{prefix}", "kraken2", "taxonomy", "nodes.dmp"),
        os.path.join("{prefix}", "kraken2", "taxonomy", "taxid.map"),
    params:
        outdir=os.path.join("{prefix}", "kraken2", "taxonomy"),
        ranks=config.args.ranks,
    log:
        os.path.join("{prefix}", "logs", "kraken2", "taxdump.log"),
    shell:
        """
        taxonkit create-taxdump -A 1 \\
        -R {params.ranks} \\
        {input} \\
        -O {params.outdir} \\
        2> {log}
        """


rule get_kraken2_taxid2lineage:
    conda:
        os.path.join(dir.envs, "taxonkit.yml")
    container:
        singularity_envs["taxonkit"]
    input:
        os.path.join("{prefix}", "kraken2", "taxonomy", "names.dmp"),
    output:
        os.path.join("{prefix}", "kraken2", "taxid2lineage.tsv"),
    params:
        os.path.join("{prefix}", "kraken2", "taxonomy"),
    shell:
        """
        export TAXONKIT_DB={params}
        taxonkit list --ids 1 | taxonkit lineage -r > {output}
        """


rule get_kraken2_formatted_headers:
    input:
        os.path.join("{prefix}", "kraken2", "taxonomy", "taxid.map"),
        os.path.join("{prefix}", "kraken2", "seqid2tax.tsv"),
    output:
        temp(os.path.join("{prefix}", "kraken2", "kraken2_headers.tsv")),
        os.path.join("{prefix}", "kraken2", "taxonomy.tsv"),
    params:
        RANKS,
    run:
        ranks = params[0].split(",")
        df = pd.read_csv(input[0], sep="\t", header=None)
        df.columns = ["seq_id", "taxid"]
        df["header"] = df.apply(
            lambda row: f"{row.seq_id}|kraken:taxid|{row.taxid}", axis=1
        )
        df[["seq_id", "header"]].to_csv(output[0], sep="\t", header=False, index=False)
        seq2tax_df = pd.read_csv(input[1], sep="\t")
        seq2tax_df.merge(df, on="seq_id")[["seq_id", "taxid"] + ranks].to_csv(
            output[1], sep="\t", index=False
        )


rule get_kraken2_formatted_fasta:
    conda:
        os.path.join(dir.envs, "seqkit.yml")
    container:
        singularity_envs["seqkit"]
    input:
        fa=os.path.join("{prefix}", "database", "amplicons.fasta"),
        tsv=os.path.join("{prefix}", "kraken2", "kraken2_headers.tsv"),
    output:
        temp(os.path.join("{prefix}", "kraken2", "kraken2.fa")),
    log:
        os.path.join("{prefix}", "logs", "kraken2", "fasta_headers.log"),
    shell:
        """
        seqkit replace \\
        -p "^(.+)$" -r "{{kv}}" \\
        -k {input.tsv} \\
        {input.fa} \\
        -o {output} \\
        2> {log}
        """


rule kraken2_build:
    conda:
        os.path.join(dir.envs, "kraken2.yml")
    container:
        singularity_envs["kraken2"]
    input:
        os.path.join("{prefix}", "kraken2", "kraken2.fa"),
    output:
        os.path.join("{prefix}", "kraken2", "hash.k2d"),
        os.path.join("{prefix}", "kraken2", "opts.k2d"),
        os.path.join("{prefix}", "kraken2", "taxo.k2d"),
    params:
        os.path.join("{prefix}", "kraken2"),
    log:
        os.path.join("{prefix}", "logs", "kraken2", "build.log"),
    shell:
        """
        kraken2-build \\
        --no-masking \\
        --add-to-library {input} \\
        --db {params} \\
        &>> {log}
        
        kraken2-build \\
        --build \\
        --db {params} \\
        &>> {log}

        kraken2-build \\
        --clean \\
        --db {params} \\
        &>> {log}
        """


rule Hash_kraken2:
    input:
        os.path.join("{prefix}", "kraken2", "taxid2lineage.tsv"),
        os.path.join("{prefix}", "kraken2", "hash.k2d"),
        os.path.join("{prefix}", "kraken2", "opts.k2d"),
        os.path.join("{prefix}", "kraken2", "taxo.k2d"),
    output:
        os.path.join("{prefix}", "kraken2", "kraken2.md5"),
    shell:
        """
        md5sum {input} | sort -k 2 | md5sum | cut -f 1 -d " " > {output}
        """


## Rules to format taxonomy to fit the requirements of the original RDP.
### Inspired from https://john-quensen.com/tutorials/training-the-rdp-classifier/ and
### following RDP authors' recommendations from https://github.com/rdpstaff/classifier/tree/17bf0bd10581f05c268e963e8c4150084d172d7d
### See 'RDP_validations.rules' for more validation based on RDP authors' recommendations


rule Preformat_for_cannonical_rdp:
    conda:
        os.path.join(dir.envs, "amplicons_r_utils.yml")
    container:
        singularity_envs["r_utils"]
    input:
        ref_tax=os.path.join("{prefix}", "database", "amplicons_tax.tsv"),
        ref_seqs=os.path.join("{prefix}", "database", "amplicons.fasta"),
    output:
        formatted_table=os.path.join("{prefix}", "rdp", "formatted_tax_table.tsv"),
    log:
        os.path.join("{prefix}", "logs", "rdp_formatted_tax_table.log"),
    threads: 1
    script:
        os.path.join(scripts, "rdp_prep_tax.R")


rule Format_rdp_lineages:
    conda:
        os.path.join(dir.envs, "python.yml")
    container:
        singularity_envs["python"]
    input:
        os.path.join("{prefix}", "rdp", "formatted_tax_table.tsv"),
    output:
        os.path.join("{prefix}", "rdp", "ready4train_lineages.txt"),
    threads: 1
    params:
        script=os.path.join(scripts, "lineage2taxTrain.py"),
    shell:
        """
        python {params.script} {input} > {output}
        """


rule Format_rdp_add_lineages:
    conda:
        os.path.join(dir.envs, "python.yml")
    container:
        singularity_envs["python"]
    input:
        tax=os.path.join("{prefix}", "rdp", "formatted_tax_table.tsv"),
        fa=os.path.join("{prefix}", "database", "amplicons.fasta"),
    output:
        read4train_fasta=os.path.join("{prefix}", "rdp", "ready4train_seqs.fasta"),
    params:
        script=os.path.join(
            scripts,
            "addFullLineage.py",
        ),
    shell:
        """
        python {params.script} {input.tax} {input.fa} > {output}       
        """


rule RDP_train:
    conda:
        os.path.join(dir.envs, "rdp_classifier.yml")
    container:
        singularity_envs["rdp_classifier"]
    input:
        fa=os.path.join("{prefix}", "rdp", "ready4train_seqs.fasta"),
        tax=os.path.join("{prefix}", "rdp", "ready4train_lineages.txt"),
    output:
        xml=os.path.join("{prefix}", "rdp", "bergeyTrainingTree.xml"),
        lst=os.path.join("{prefix}", "rdp", "genus_wordConditionalProbList.txt"),
        idx=os.path.join("{prefix}", "rdp", "wordConditionalProbIndexArr.txt"),
        wrd=os.path.join("{prefix}", "rdp", "logWordPrior.txt"),
    params:
        mem=RDP_MEM,
        out=os.path.join("{prefix}", "rdp"),
    threads: 1
    resources:
        mem_mb=get_mem_mb(RDP_MEM),
    shell:
        """
        rdp_classifier \\
        -Xmx{params.mem} -XX:ConcGCThreads={threads} \\
        train \\
        -o {params.out} \\
        -s {input.fa} \\
        -t {input.tax} 
        """


rule get_RDP_properties:
    input:
        xml=os.path.join("{prefix}", "rdp", "bergeyTrainingTree.xml"),
        lst=os.path.join("{prefix}", "rdp", "genus_wordConditionalProbList.txt"),
        idx=os.path.join("{prefix}", "rdp", "wordConditionalProbIndexArr.txt"),
        wrd=os.path.join("{prefix}", "rdp", "logWordPrior.txt"),
    output:
        os.path.join("{prefix}", "rdp", "rRNAClassifier.properties"),
    params:
        xml=lambda wildcards, input: os.path.basename(input.xml),
        lst=lambda wildcards, input: os.path.basename(input.lst),
        idx=lambda wildcards, input: os.path.basename(input.idx),
        wrd=lambda wildcards, input: os.path.basename(input.wrd),
        ver="RDP Naive Bayesian rRNA Classifier Version 2.14, August 2023",
    shell:
        """
        echo "bergeyTree={params.xml}
        probabilityList={params.lst}
        probabilityIndex={params.idx}
        wordPrior={params.wrd}
        classifierVersion={params.ver}" > {output}
        """


rule Hash_RDP:
    input:
        os.path.join("{prefix}", "rdp", "ready4train_seqs.fasta"),
        os.path.join("{prefix}", "rdp", "ready4train_lineages.txt"),
        os.path.join("{prefix}", "rdp", "bergeyTrainingTree.xml"),
        os.path.join("{prefix}", "rdp", "rRNAClassifier.properties"),
    output:
        os.path.join("{prefix}", "rdp", "rdp.md5"),
    shell:
        """
        md5sum {input} | sort -k 2 | md5sum | cut -f 1 -d " " > {output}
        """


rule Decipher_prep_fasta:
    conda:
        os.path.join(dir.envs, "amplicons_r_utils.yml")
    container:
        singularity_envs["r_utils"]
    input:
        ref_tax=os.path.join("{prefix}", "database", "amplicons_tax.tsv"),
        ref_seqs=os.path.join("{prefix}", "database", "amplicons.fasta"),
    output:
        decipher_seqs=os.path.join(
            "{prefix}", "decipher", "Decipher_DB_amp_taxonomy.fasta"
        ),
    log:
        os.path.join(
            "{prefix}", "logs", "decipher", "DB_amp_taxonomy_decipher_fasta.log"
        ),
    threads: 1
    script:
        os.path.join(scripts, "decipher_prep_tax.R")


rule Decipher_train_tax:
    conda:
        os.path.join(dir.envs, "decipher.yml")
    container:
        singularity_envs["decipher"]
    input:
        decipher_seqs=os.path.join(
            "{prefix}", "decipher", "Decipher_DB_amp_taxonomy.fasta"
        ),
    output:
        trained_tax=os.path.join(
            "{prefix}", "decipher", "Decipher_DB_amp_taxonomy_trained_tax.rds"
        ),
        training_plot=os.path.join(
            "{prefix}", "decipher", "Decipher_DB_amp_taxonomy_trained_plot.pdf"
        ),
    log:
        os.path.join(
            "{prefix}", "logs", "decipher", "DB_amp_taxonomy_decipher_tax_tree.log"
        ),
    threads: 1
    script:
        os.path.join(scripts, "decipher_train_tax.R")


rule Hash_Decipher:
    input:
        os.path.join("{prefix}", "decipher", "Decipher_DB_amp_taxonomy_trained_tax.rds"),
    output:
        os.path.join("{prefix}", "decipher", "decipher.md5"),
    shell:
        """
        md5sum {input} | sort -k 2 | md5sum | cut -f 1 -d " " > {output}
        """
