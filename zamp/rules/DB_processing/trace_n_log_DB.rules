## Set of rules to backup the input database and generate a hash code of these input files and the output files.


rule backup_original_files:
    input:
        fa=FASTA,
        tax=TAXONOMY,
    output:
        fa=os.path.join("{prefix}", "database", "original_seqs.fasta"),
        tax=os.path.join("{prefix}", "database", "original_tax.txt"),
    threads: 1
    shell:
        """
        cp {input.fa} {output.fa}
        cp {input.tax} {output.tax}
        """


rule clean_taxonomy:
    conda:
        os.path.join(dir.envs, "pandas.yml")
    container:
        singularity_envs["pandas"]
    input:
        os.path.join("{prefix}", "database", "original_tax.txt"),
    output:
        temp(os.path.join("{prefix}", "database", "clean_tax.tsv")),
    params:
        db_name=DBNAME,
    script:
        os.path.join("scripts", "clean_tax.py")


rule Hash_original_files:
    input:
        os.path.join("{prefix}", "database", "original_seqs.fasta"),
        os.path.join("{prefix}", "database", "original_tax.txt"),
    output:
        os.path.join("{prefix}", "database", "original.md5"),
    shell:
        """
        md5sum {input} | sort -k 2 | md5sum | cut -f 1 -d " " > {output}
        """


## Generate a hash from all relevant files to insure that we always work with the same reference databases

hashes = {
    "rdp": os.path.join("{prefix}", "rdp", "rdp.md5"),
    "qiime2": os.path.join("{prefix}", "qiime2", "qiime2.md5"),
    "dada2": os.path.join("{prefix}", "dada2", "dada2.md5"),
    "sintax": os.path.join("{prefix}", "sintax", "sintax.md5"),
}

hash_out = [os.path.join("{prefix}", "database", "original.md5")]
for classifier in CLASSIFIERS:
    hash_out.append(hashes[classifier])


rule hash_global_DB:
    input:
        hash_out,
    output:
        os.path.join("{prefix}", "database.md5"),
    threads: 1
    shell:
        """
        md5sum {input} | sort -k 2 | md5sum | cut -f 1 -d " " > {output}
        """
