## Set of rules to backup the input database and generate a hash code of these input files and the output files.


rule Back_up_master:
    input:
        seqs=FASTA,
        tax=TAXONOMY,
    output:
        seqs=os.path.join("{prefix}", "master", "original_seqs.fasta"),
        tax=os.path.join("{prefix}", "master", "original_tax.txt"),
    threads: 1
    shell:
        """
        cp {input.seqs} {output.seqs} && \
        cp {input.tax} {output.tax}
        """


rule Hash_master_files:
    input:
        os.path.join("{prefix}", "master", "original_seqs.fasta"),
        os.path.join("{prefix}", "master", "original_tax.txt"),
    output:
        os.path.join("{prefix}", "master", "original.hash"),
    threads: 1
    shell:
        """
        md5sum {input} | sort -k 2 | md5sum | cut -f 1 -d " " > {output}
        """


## Generate a hash from all relevant files to insure that we always work with the same reference databases

hashes = {
    "rdp": os.path.join("{prefix}", "RDP", "RDP_DB.hash"),
    "qiimerdp": os.path.join("{prefix}", "QIIME", "DB_formatted.hash"),
    "dada2rdp": os.path.join("{prefix}", "dada2rdp", "DADA2_DB.hash"),
    "decipher": os.path.join("{prefix}", "RDP", "RDP_DB.hash"),
}

hash_out = [os.path.join("{prefix}", "master", "original.hash")]
for classifier in CLASSIFIERS:
    hash_out.append(hashes[classifier])


rule hash_global_DB:
    input:
        hash_out,
    output:
        os.path.join("{prefix}", "DB.hash"),
    threads: 1
    shell:
        """
        md5sum {input} | sort -k 2 | md5sum | cut -f 1 -d " " > {output}
        """
