## Set of rules to backup the input database and generate a hash code of these input files and the output files.


rule Back_up_master:
    input:
        fa=FASTA,
        tax=TAXONOMY,
    output:
        fa=os.path.join("{prefix}", "master", "original_seqs.fasta"),
        tax=os.path.join("{prefix}", "master", "original_tax.txt"),
    threads: 1
    shell:
        """
        cp {input.fa} {output.fa}
        cp {input.tax} {output.tax}
        """


rule clean_taxonomy:
    conda:
        os.path.join(dir.envs, "pandas.yml")
    container:
        singularity_envs["pandas"]
    input:
        os.path.join("{prefix}", "master", "original_tax.txt"),
    output:
        temp(os.path.join("{prefix}", "QIIME", "clean_tax.tsv")),
    params:
        db_name=DBNAME,
    script:
        os.path.join("scripts", "clean_tax.py")


rule Hash_master_files:
    input:
        os.path.join("{prefix}", "master", "original_seqs.fasta"),
        os.path.join("{prefix}", "master", "original_tax.txt"),
    output:
        os.path.join("{prefix}", "master", "original.hash"),
    threads: 1
    shell:
        """
        md5sum {input} | sort -k 2 | md5sum | cut -f 1 -d " " > {output}
        """


## Generate a hash from all relevant files to insure that we always work with the same reference databases

hashes = {
    "rdp": os.path.join("{prefix}", "RDP", "RDP_DB.hash"),
    "qiimerdp": os.path.join("{prefix}", "QIIME", "DB_formatted.hash"),
    "dada2rdp": os.path.join("{prefix}", "dada2rdp", "DADA2_DB.hash"),
    "decipher": os.path.join("{prefix}", "RDP", "RDP_DB.hash"),
}

hash_out = [os.path.join("{prefix}", "master", "original.hash")]
for classifier in CLASSIFIERS:
    hash_out.append(hashes[classifier])


rule hash_global_DB:
    input:
        hash_out,
    output:
        os.path.join("{prefix}", "DB.hash"),
    threads: 1
    shell:
        """
        md5sum {input} | sort -k 2 | md5sum | cut -f 1 -d " " > {output}
        """
