import pandas as pd
import re
import os


if not LOCAL:

    rule get_assembly_finder_input:
        output:
            temp(os.path.join(dir.out.base, "assembly_finder", "input.tsv")),
        params:
            queries=INPUT.split(","),
            taxon=TAXON,
        run:
            if not os.path.isfile(INPUT):
                if params.taxon:
                    pd.DataFrame.from_dict({"taxon": params.queries}).to_csv(
                        output[0],
                        sep="\t",
                        index=False,
                    )
                else:
                    pd.DataFrame.from_dict({"accessions": params.queries}).to_csv(
                        output[0], sep="\t", index=False, header=False
                    )
            else:
                df = pd.read_csv(os.path.abspath(INPUT), sep="\t", header=None)
                if params.taxon:
                    df.columns = ["taxon"]
                    df.to_csv(
                        output[0],
                        sep="\t",
                        index=False,
                    )

                else:
                    df.columns = ["accession"]
                    df.to_csv(output[0], sep="\t", index=False, header=False)

    if not TAXON:
        AF_ARGS += "--accession "
    if TAXON:
        if LIMIT != "all":
            AF_ARGS += f"--limit {LIMIT} "
        if ASM_LEVEL:
            AF_ARGS += f"--assembly-level {ASM_LEVEL} "
        if ONLY_REF:
            AF_ARGS += f"--reference True "
        elif not ONLY_REF:
            AF_ARGS += f"--reference False "
        if RANK and NRANK:
            AF_ARGS += f"--rank {RANK} --nrank {NRANK} "

    checkpoint download_assemblies:
        conda:
            os.path.join(dir.envs, "assembly_finder.yml")
        container:
            singularity_envs["assembly_finder"]
        input:
            os.path.join(dir.out.base, "assembly_finder", "input.tsv"),
        output:
            asm=os.path.join(dir.out.base, "assembly_finder", "assembly_summary.tsv"),
            seq=os.path.join(dir.out.base, "assembly_finder", "sequence_report.tsv"),
            tax=os.path.join(dir.out.base, "assembly_finder", "taxonomy.tsv"),
        params:
            args=AF_ARGS,
            out=os.path.join(dir.out.base, "assembly_finder"),
        log:
            os.path.join(dir.logs, "assembly_finder.log"),
        threads: 3
        shell:
            """
            assembly_finder \\
            --no-use-conda \\
            -i {input} \\
            --threads {threads} \\
            {params.args} \\
            -o {params.out} 2> {log}
            """

else:
    fastas = []
    for ext in ["fna", "fasta", "fa"]:
        fastas.extend(glob.glob(os.path.join(config.args.input, f"*.{ext}*")))
    suffix = ".".join(fastas[0].split(".")[-2::])


rule in_silico_PCR:
    conda:
        os.path.join(dir.envs, "perl.yml")
    container:
        singularity_envs["perl"]
    input:
        get_fasta,
    output:
        os.path.join(dir.out.base, "in_silico_pcr", "{sample}.fasta"),
    log:
        os.path.join(dir.logs, "in_silico_pcr", "{sample}.log"),
    params:
        fw=FW_PRIMER,
        rv=RV_PRIMER,
        script=os.path.join(
            workflow.basedir, "rules", "In_silico", "scripts", "in_silico_PCR.pl"
        ),
    shell:
        """
        perl {params.script} \\
        -s {input} \\
        -a {params.fw} \\
        -b {params.rv} \\
        -m -r -e \\
        2> {output} > {log}
        """


### Combine all extracted sequences in one big fasta
rule Insilico_merge_all_in_one_fasta:
    input:
        list_amplicons,
    output:
        temp(os.path.join(dir.out.base, "in_silico_pcr", "all_amplicons.fna")),
    shell:
        """
        cat {input} > {output}
        """


### Again, dereplicate all identical sequences after merging.
rule vsearch_derepicate_amplicons:
    conda:
        os.path.join(dir.envs, "vsearch.yml")
    container:
        singularity_envs["vsearch"]
    input:
        os.path.join(dir.out.base, "in_silico_pcr", "all_amplicons.fna"),
    output:
        os.path.join(dir.out.base, "vsearch", "2_denoised", "dna-sequences.fasta"),
    log:
        os.path.join(dir.logs, "vsearch", "in_silico.log"),
    shell:
        """
        vsearch \\
        --derep_fulllength {input} \\
        --minuniquesize 1 \\
        --relabel Seq_ \\
        --output {output} \\
        2> {log}
        """


### Count the number of occurences of the representative sequences in the samples.


rule InSilico_count_occurences:
    conda:
        os.path.join(dir.envs, "vsearch.yml")
    container:
        singularity_envs["vsearch"]
    input:
        samples=os.path.join(dir.out.base, "in_silico_pcr", "{sample}.fasta"),
        rep_seq=os.path.join(
            dir.out.base, "vsearch", "2_denoised", "dna-sequences.fasta"
        ),
    output:
        os.path.join(
            dir.out.base,
            "vsearch",
            "2_denoised",
            "countSeqs",
            "{sample}_count_table.tsv",
        ),
    log:
        os.path.join(
            dir.logs, "vsearch", "2_denoised", "countSeqs", "{sample}_count_table.tsv"
        ),
    shell:
        """       
        if [ -s "{input[samples]}" ]
        then
            echo "{input[samples]} has some data." && \
            vsearch --usearch_global {input[samples]} \
                -otutabout {output} \
                -id 1 \
                -strand plus \
                --db {input[rep_seq]} 
                # do something as file has data
        else
            echo "{input[samples]} is empty." && \
            touch {output}
            # do something as file is empty
        fi 2> {log}        
        """


### Format count table from InSilico
rule create_InSilico_count_table:
    conda:
        os.path.join(dir.envs, "amplicons_r_utils.yml")
    container:
        singularity_envs["r_utils"]
    input:
        count_table_samples=list_samples_counts,
    output:
        count_table=os.path.join(
            dir.out.base, "vsearch", "2_denoised", "count_table.tsv"
        ),
    log:
        os.path.join(dir.logs, "vsearch", "2_denoised", "count_table.tsv"),
    script:
        os.path.join("scripts", "create_count_table_from_insilico.R")


### Create a table to compare tax assignment
rule In_silico_tax_compare:
    conda:
        os.path.join(dir.envs, "pandas.yml")
    container:
        singularity_envs["pandas"]
    input:
        db_tax=os.path.join(DBPATH, "{tax_DB}", "database", "amplicons_all_tax.tsv"),
        count_table=os.path.join(
            dir.out.base, "vsearch", "2_denoised", "count_table.tsv"
        ),
        expected_taxonomy=get_tax_table,
        assigned_taxonomy=os.path.join(
            dir.out.base,
            "vsearch",
            "3_classified",
            "{classifier}_{tax_DB}",
            "dna-sequences_tax_assignments.txt",
        ),
    output:
        amplicons=os.path.join(
            dir.out.base,
            "vsearch",
            "3_classified",
            "{classifier}_{tax_DB}",
            "amplicons_tax_compare.tsv",
        ),
        assemblies=os.path.join(
            dir.out.base,
            "vsearch",
            "3_classified",
            "{classifier}_{tax_DB}",
            "assemblies_tax_compare.tsv",
        ),
    log:
        os.path.join(
            dir.logs,
            "vsearch",
            "3_classified",
            "{classifier}_{tax_DB}",
            "InSilico_compare_tax.log",
        ),
    params:
        local=LOCAL,
        suffix=suffix,
    script:
        os.path.join("scripts", "tax_compare.py")


rule get_classification_metrics:
    conda:
        os.path.join(dir.envs, "pandas.yml")
    container:
        singularity_envs["pandas"]
    input:
        os.path.join(
            dir.out.base,
            "vsearch",
            "3_classified",
            "{classifier}_{tax_DB}",
            "amplicons_tax_compare.tsv",
        ),
    output:
        os.path.join(
            dir.out.base,
            "metrics",
            "{classifier}_{tax_DB}_all_scores.tsv",
        ),
        os.path.join(
            dir.out.base,
            "metrics",
            "{classifier}_{tax_DB}_mean_scores.tsv",
        ),
    params:
        script=os.path.join(
            workflow.basedir, "rules", "In_silico", "scripts", "get_f1_scores.py"
        ),
        prefix=os.path.join(
            dir.out.base,
            "metrics",
            "{classifier}_{tax_DB}",
        ),
    shell:
        """
        python {params.script} {input} {params.prefix}
        """


rule merge_classification_metrics:
    input:
        expand(
            os.path.join(
                dir.out.base,
                "metrics",
                "{classifier}_{tax_DB}_mean_scores.tsv",
            ),
            classifier=CLASSIFIER,
            tax_DB=DBNAME,
        ),
    output:
        os.path.join(
            dir.out.base,
            "all_scores.tsv",
        ),
    run:
        dfs = []
        ranks = list(pd.read_csv(input[0], sep="\t")["rank"].drop_duplicates())
        for tsv in input:
            meta = os.path.basename(tsv).split("_mean_scores.tsv")[0]
            df = pd.read_csv(tsv, sep="\t")
            classifier = meta.split("_")[0]
            df["classifier"] = [classifier] * len(df)
            df["database"] = [meta.split(f"{classifier}_")[1]] * len(df)
            dfs.append(df)
        dfs = pd.concat(dfs)
        dfs["rank"] = pd.Categorical(
            dfs["rank"],
            categories=ranks,
            ordered=True,
        )
        dfs.sort_values(["rank", "f1_score"], ascending=[True, False]).to_csv(
            output[0], sep="\t", index=False
        )
